+---
 +layout: post
 +title: "Is Artificial Intelligence Just Automation?"
 +date: 2016-09-07
 +categories: ai
 +---
 
 To many, "Artificial Intelligence" is an exciting concept.  It's sexy.  It's scary.  It's futuristic.  By contrast, "automation" sounds dull, mechanistic, and Orwellian.  It sounds like mindless machines, taking away what soul we have left on planet Earth.  There's an emotional difference between the two terms, in other words.  But is there a real difference?  What is AI?  How exactly is it different than automation?
 
 I think the confusion between AI and automation is central to a broader cultural and intellectual confusion taking place today.  Futurists such as Ray Kurzweil, or Elon Musk, or Kevin Kelly, or Oxford professor Nick Bostrom (and there are many others) imbue AI with almost magical powers.  AI's will "come alive" and have minds, and formulate devious plans to take us over.  To the AI-Futurists, we are in the process of a profound transformation, amounting to the evolution of a new species.  Like Kubrick's (and Spielberg's) 2001 movie A.I., we are "Orga" or organic life forms, and AI's are "Mecha" or mechanical life forms.  But the AI's keep getting smarter and smarter, and so the distinction between mindful Orgas and mechanical Mechas eventually disappears.  To the real futurist, ther world belongs to the AIs.   The evil genius in the 2015 hit Ex Machina captures the futurist mood, when he laments that the prospect of AIs overtaking mere humans is inevitable.  "One day the AIs are going to look back on us the same way we look at fossil skeletons on the plains of Africa," he says, taking a drink of vodka, musing existentially about his life.
 
 Well futurist AI is sexy, after all, in the same way that good fiction generally has an emotional appeal.  Let's hope it continues.  But the question of what AI is, scientifically, I think is somewhat different.  At the heart of the question of AI is not all the sci-fi futurism about minds and motives and new, sentient Mechas "coming alive" but rather the boring Orwellian question of automation.  For, AI today is just automation, and nothing else.  Can automation become mindlike?
 
 To be sure, Kurzweil and his legions of Futurist-AI enthusiasts are quick to point out that current AI applications are "narrow," meaning that they only solve problems in a specific application domain.  Narrow AI is, well, AI:  chess playing computers only play chess; Jeopardy! playing computers only play Jeopardy!; self-driving cars only drive cars.  So the immediate problem arises as to how the Narrow AI we see around us today will sprout into the Ex Machina type of AI, what we used to call Strong AI, and now refer to as "Artificial General Intelligence."  How does this transformation happen?
 
 Again, listening to a technology evolutionist like Kurzweil or co-founder of Wired magazine Kevin Kelly (check out his 2010 book What Technology Wants), the transition from Narrow to General AI happens sort of like falling in love:  first gradually, then all at once.  AI gains in sophistication, chugging along as Narrow AI, until it reaches a level of intelligence, at which point it starts designing itself and becomes "ultra intelligent," as the late mathematician I.J. Good famously proposed back in the 1960s.  More recently, Nick Bostrom, author of the 2012 best-seller Superintelligence, proposed the same idea, calling it now superintelligence.
 
 This is all great futurism, but it doesn't explain anything.  Sure, AI becomes more powerful.  But the question is how it becomes less narrow.  Or:  how does AI--which only works with specific applications--somehow come to exhibit general intelligence, like a mindful creature, like us?  Confidently asserting that intelligence keeps increasing or evolving to a point where it explodes into superhuman intelligence doesn't tell us anything much.
 
 In facdt, we can drop out of this discussion the blather about superintelligence; it's just general human intelligence we're concerned with.  So how does that happen?  How do computers start acting like human minds?  To see why this is actually a very deep question, and one without an easy answer at all (no matter what the AI-Futurists insist), we have to step back and look at how boring, ordinary Narrow AI works.  Why start here?  Because it's the only AI we actually have.  All the speculation about the other kind--the Artificial General Intelligence (AGI)--is just that.  We have to start with what AI is, and try to understand where it might be headed.  If this sounds overly conservative, I'd argue that it's much more scientifically minded than starting with what we want it to be (futurism), then insisting that it will be that way.  Let's start with observation, in other words.
 
 Building an AI application involves two steps.  One, the problem one is trying to solve, or analyze, must be representable.  If I want to teach a computer to play the ancient Chinese game of Go, I must first figure out how to represent the game of Go not as a human sees it, but rather as a data structure, where computation can be performed on it.  Otherwise, no AI.
 
 Second, the problem must be analyzable in terms of a sequence of steps.  We can call this algorithmic, or just computational.  Every problem that admits of an AI solution (or any computational solution) must first be translated into a representation, with which the computation can then be specified and performed.  If not, there's no AI.
 
 Using this simple framework, it's easy to see how Narrow AI keeps succeeding, at more and more complicated tasks.  Engineers and scientists, armed with more and more powerful computers, and using a larger and larger stock of techniques and approachews from prior attempts at other problems, keep finding creative ways to represent pieces of our world as computational problems.  Strategy board games like checkers, chess, and now Go are classic examples of this process of reducing a bit of intellectual activity (playing chess, say) to a computational representation.  First, the game of chess is viewed as a data structure--a game tree--, then algorithms are defined on the game tree such that good or bad (and legal or illegal) moves can be taken, or simulated, on the computational representation of the game.  Once we have this reduction to computation, depending on the complexity of the problem and other factors influencing difficuly, we can start building an AI.
 
 A Narrow AI.  Because the problem reduced in this way is in essense reduced to a sequence of steps, the AI that succeeds at, say, playing great chess typically does nothing else.  The representation was brilliant for defining and devising chess algorithms like min-max, but by virtue of the specifics of the reduction and the algorithms defined on top of it, lots of other intellectual stuff gets pushed out of reach.  Narrow AI makes AGI less likely.  This is why IBM's Watson raised such a stir by playing Jeopardy!, but somehow failed to understand much of anything else about language, especially the more general, simple tasks of reading and understanding basic prose, like in the newspaper.
 
 In fact, much of the intelligence of an AI system like Watson can be attributed to the human engineers, who ingenioiusly found ways to make playing the game of Jeopardy non-general.  They found an architecture, and a set of algorithms, that tranlate the task of playing Jeopardy! into a tractable representation and computation.  At that point, all the intelligence is "set" in the system by the reduction; Watson isn't gonna run off and start thinking or doing something else.  That's the price of Narrow AI victory.
 
 Similar comments could be made about any other Narrow AI application, including voice to text, machine translation, self-driving cars, personal assistants like Google Now, and anything else that so amazes us today.  It's all a reduction to representation and computation.  And it follows, given this I think relatively straightforward treatment of how AI actually works today, that it's quite a leap of imagination to imagine any of these systems "coming alive."  In fact, once one understands how they exhibit the simulated intelligence that they do, it becomes even more difficult to see how they can become general, or mind-like, at all.  It's apples and oranges, contra Kurzweil and others who insist that today's successes are stepping stones to tomorrow's Mecha minds.  But how?  (Hey, it's sci-fi dreaming, don't ask how.)
 
 At this point, we can just throw out all this Narrow AI stuff if we wish (it's not aiding the discussion of AGI, anyway), and ask how AI might nevertheless arrive at general intelligence.  Forget Narrow AI.  What other paths exist?  And I thin this is where things get more interesting.
 
 Science is great at finding limits to knowledge, especially when it's attempting to extend our knowledge.  The laws of dynamics tell us about entropy; Einstein showed that the speed of light had a limit; quantum physics tells us about uncertainty; chaos and complexity tells us about unpredictability; mathematical logic tells us about undecideability.  This is all part and parcel of good science.  We learn not just what we can do, but what we can't do.  And I think the lesson of Narrow AI is that we learn that what makes AI succeed, when it does, is a specific representation and computation strategy.  It's in the reduction of the messy problem to structure and algorithm that anything interesting can happen.  When we ask then how to make the intelligent simulation general, too, we're asking how we can escape the limits or boundaries we put on the problem in order to, well, make it work.  So this is a very basic problem for the futurists.  While they point to Narrow AI success as evidence for imminent AGI, they use examples that shine no light on AGI at all.  They're pointing to examples of limits to general intelligence, in order to argue for it.
 
 Another way to understand the distinction between the usable AI we see around us, Narrow AI, and the futuristic AI we talk about in movies and on blogs (not this one), is by asking if there are aspects of our intelligent life that don't seem reducible to specific representations or computations.  There are, of course, and we don't have to look far at all.  We can look no further than ordinary, everyday language, like a conversation you might have with a six year old, or whomever.  In a later post, I'll explain why natural langauge understanding is not a Narrow AI application, and what this suggests about the real differences between Orga and Mecha.
 
 
 
