---
layout: post
title: "Prolegomena to a Digital Humanism"
date: 2014-01-28
categories: 
---

"What makes something fully real is that it's impossible to represent it to
completion." - Jaron Lanier

The entire modern world is inverted. In-Verted. The modern world is the story
of computation (think: the internet), and computation is a representation of
something real, an abstraction from real particulars. The computation
representing everything and connecting it together on the internet and on our
digital devices is now more important to many of us than the real world. I'm
not making a retrograde, antediluvian, troglodyte, Luddite point; it's _deep_ ,
what I'm saying. It's hard to say clearly because my limited human brain
doesn't want to wrap around it. (Try, try, try &mdash; if only some computation
would help me. But alas.)

The modern world is _inverted_ to the extent that abstractions of reality
become more important than the real things. A computer representation of an oil
painting is not an oil painting. Most people think the representation is the
wave of the future. The oil painting is, actually.

What's a computer representation of a person? This is the crux of the problem. 
To understand the problem we have to understand two big theory problems here,
and I'll be at some pains to explain them. First, suppose I represent _you_ 
&mdash; or "model" you, in the lingo &mdash; in some software code. Suppose for
example I model all the employees working at a company because I want to predict
who fits best for a business project happening in, say, Europe (It's a big
corporation with huge global reach and many business units, like IBM. No one
really knows all the employees and who's qualified for what, except locally
perhaps. The example is real-world). That necessarily means I have a thinner
copy of the "real" you&mdash; I may not know you at all, so I'm abstracting away
some data stored in a database about you&mdash;your position, salary, latest
performance reports, the work you do, a list of job skills. Because
abstractions are simplified models of real things, they can be used to do big
calculations (like a database operation that returns all the people who know
C++); it also means they leave out details. Abstractions conceived of as
accurate representations are a lie, to put it provocatively, as the philosopher
Nietzsche remarked once (He said that Words Lie. You say "this is a leaf",
pointing to a leaf. There is no such thing as a "leaf" as an abstract concept. 
There are only leaves...). 

_All representations are in a language, and every language has limits to its
expressiveness. Natural language like English is most expressive, which is why
a novel or a poem can capture more about the human experience than mathematics
can, or computer code. This point is lost on many Silicon Valley 'Singularity'
types&mdash;technologists and futurists who want computation to replace the
messy real word._ 

Change the example if you want, because abstracting the real world and
especially human behavior into slick computer models is all the rage today. 
Examples abound. Say I go shopping at the same store. I shop at Safeway. 
Whenever I go to Safeway, I get a bunch of coupons when I check out at the self
check out. The coupons are strangely relevant&mdash;I get deals on protein bars
and chocolate milk and so on. Funny thing is that I buy all those items, but I
didn't necessarily buy any of the coupon items when I received those coupons. 
What's happening here is that Safeway has made a model of "me" in its databases,
and it runs some simple statistics on my purchases as a function of time (like: 
month by month by item type, say), and from this data it makes recommendations. 
People like this sort of service, generally. You talk to technologists and the
people who're modernizing the consumer experience and you'll get a vision of our
future: walk into the supermarket and scan your ID into the cart. It starts
directing you to items you need, and recommending other items "I noticed you
bought tangerines the other day, you might like tangelos too. They're on sale
today on Aisle 5." 

Now, nothing is wrong here, it's just a "lie" of sorts is all. I'm generally
not my Safeway model is all. Not completely. The model of me that Safeway has
is based on my past buying patterns, so if I change anything or if the world
changes, it's suddenly irrelevant and it starts bugging me instead of helping
me. It's a lie, like Nietzsche said, and so it gets out of sync eventually with
the actual me that's a real person. I don't buy chocolate but on Valentines Day
I do, say. Or I'm always buying ice cream but last week I started the Four Hour
Body diet and so now I only buy that on Saturdays, and I buy beans all the time
now. But right when I get sick of them and start buying lentils the system has
a good representation of me as a bean-buyer, so now I'm getting coupons for
beans at precisely the time I'm trying to go no-beans (but I'm still into
legumes). Or I'm running errands for someone else, who loves Almond Milk. 
Almond Milk is on sale but I don't get that information; I only see 2% Lactose
Free milk is on sale because I usually buy that. The more the model of me is
allowed to lord over me, too, the worse things get. If the cart starts pulling
me around to items that I "need", and it's wrong, I'm now fighting with a
physical object &mdash; the shopping cart &mdash; because it's keeping me from
buying lentils and Almond Milk. None of this has happened yet, but welcome to
creating mathematical objects out of real things. The computer can't help with
any of my buying behavior _today_ , because it's got a stale, simple model of
me based on my buying behavior _yesterday_ . That's how computers work. 
(Would it be a surprise to learn that the entire internet is like this? I mean:
 a shallow, stale, simple model of everything? Well, it is. Read on.)

Let's finish up with abstraction. Someone like Mathew Crawford, who wrote the
best-selling "Shop Class as Soul Craft", dropped out of a six figure a year job
at a think tank writing about politics in D.C. to fix motorcycles, because when
he realized the modern world is inverted, and abstractions are becoming more
important than real things and experiences, he was desperate to find something
meaningful. He wasn't persuaded, like the Silicon Valley culture seems to be,
that all these abstractions are actually getting smarter and smarter and making
us all better and better. He opened a motorcycle repair shop in Virginia and
wrote a book about how you can't rely on abstractions and be any good at all
fixing real things like motorcycles.

This is an interesting point, actually. Crawford's an interesting guy. You
could write a dissertation alone on how difficult it is to diagnose and fix
something complicated. You can download instructions and diagnostics from the
internet, but you're not a real mechanic if you don't feel your way through the
problem. Computation is supposed to replace all of this embarrassing human
stuff like intuition and skill and judgement. "Feeling our way" through things
is supposed to be in the past now, and really the pesky "human element" is
supposed to go away too. 

A confusion of the modern inverted age is that as computers get smarter (but
they don't, not like people do), we're supposed to get smarter and better, too. 
But all this sanguine optimism that everything is getting "smarter" disguises
the truth, which is that it's impossible for us to get "smarter" by pretending
that computers are smarter&mdash;we have to choose. For example, if we pretend
that abstractions are "smart", we have to fit into them to continue the
illusion. If we start imposing the messy reality of life onto things, the
abstractions will start looking not-so-smart, and then the entire illusion is
gone. Poof! To the extent that we can't handle exposing our illusions, we're
stooping down to accommodate them. All this becomes clear when you open a
motorcycle repair shop and discover that you have to feel your way through the
problem and abstractions of fixes don't really help.

So much for Crawford. There are so many Crawford's today, actually. I think
it's time to start piecing together the "counter-resistance" to what Lanier
calls the Digital Maoists or Cybernetic Totalists&mdash;the people saying that
the abstractions are more real and smart than what's actually real and smart. 
The people saying the human element is old news and not important. The people
saying that the digital world is getting smarter and coming alive. If it sounds
crazy (and it should), it's time to start pointing it out.

I can talk about Facebook now because I don't like Facebook at all and almost
everyone I know seems to be obsessed with it. (It makes me reluctant to
complain too much or when I'm aggravated it emboldens me to complain with a kind
of righteous indignation.) Facebook is a model of you for the purposes of
telling a story about you. Who is reading the story and why? On the internet
this is called "sharing" because you connect to other models called "friends"
and information about your model is exchanged with your Friend-Models. Mostly,
this trickles down to the actual things&mdash;the people&mdash;so that we feel a
certain way and receive a certain satisfaction. It's funny that people who use
Facebook frequently and report that they have many friends on Facebook also
report greater degrees of loneliness in the real world. Which way does the
arrow of causality go? Were they lonely types of people first? Or does
abstraction into shallow models and using emotional words like "friends" and
"social" make their actual social existence worse somehow? 

I don't think there's much wrong with a shallow Facebook model of me or you,
really. Facebook started out as a way to gawk at attractive nineteen year old
Harvard women, and if you want to do this, you need an abstraction that
encourages photo sharing. I don't necessarily want this experience to be deep
either. I don't want three hundred _friends_ to have a deep model of me
online, necessarily, either.

Theoretically, though, the reason Facebook models are shallow is the same reason
that Safeway only wants my buying behavior in my Supermarket Model. Since
"Facebook" is really a bunch of servers (a "server" is a computer that services
other computers&mdash;it's a computer is all), then what the real people who own
Facebook can do is determined by what Facebook computers can do, with our
models. Since computers are good at doing lots of shallow things quickly (think
the Safeway database), why would Facebook have rich models of us? Then they
couldn't do much with them. It's an important but conspiratorial-sounding point
that most of what Facebook wants to do with your Facebook model, connected to
your Facebook friend models, is run statistics on what ads to sell you. It's
another significant but bomb-shell type of observation here that all the
supposed emerging smartness of the World Wide Web is laser-focused on targeted
advertising. All this liberation we think we feel is really disguising huge
seas of old-fashioned _persuasion_ and _advertising_ . Because everything we
get online is (essentially) free &mdash; think Facebook &mdash; it's no wonder
that actual money will be concentrated on ads. (Where does the the actual
earned money come from still, to buy the advertised goods and services? That
gets us back into the messy real world.)

So much for abstraction. Let's say that abstraction is often shallow, and even
vapid. It's incomplete. This says something true. It means that we ought to
think of computation as _a shallow but convenient_ way to do lots of things
quickly. We shouldn't confuse it with life. Life here includes the mystery of
human life: our consciousness, our thoughts, and our culture. We confuse
abstractions with the fullness of life at our peril. It's interesting to ask
what vision of digital technology would support a better cultural experience, or
whether shallow, infantile, ad-driven models are the best we can do. Maybe why
we cheer lead so loudly for Facebook and the coming "digital revolution" is
because we think it's the only way things can turn out, and it's better than
horses and buggies. This is a really unfortunate way to think about technology
and innovation I would say...

<span style="font-size: large;"> T </span> he second way the modern world is
inverted&mdash; the second theoretical problem with treating computer models as
reality &mdash; is known as the Problem of Induction (POI). Someone like former
trader Nasam Nicholas Taleb describes the POI as the problem of the Black Swan. 
Most swans &mdash; the vast majority of swans &mdash; are white, so eventually
you generalize in your code or your database or your mind to think something
like "All swans are white." Taleb calls this a Gaussian distribution (or normal
distribution) because you don't expect there to be outliers that screw
everything up. Taleb says that sometimes the real events in the world are not
so much like Gaussian distributions but are like exponential ones. He calls
this the Black Swan phenomenon. It's tied to the ancient POI as I'll explain. 
I mean: when a Black Swan shows up and we all thought "All swans are white."

We'll say first that a Gaussian or normal distribution is like the height of
people in the real world. Most people are between 5 and 6 feet tall, the vast
majority in fact. This is a normal distribution. It's rare to have a 7' guy or
a 4' one, and essentially impossible to have a 9' one or a 3' one. If human
height was like an Exponential Distribution, though, or a "Black Swan", then
occasionally there'd be a guy that was a hundred feet tall. He'd be rare, but
unlike with the Gaussian, he'd be guaranteed to show up one day. This would
screw something up no doubt, so it's no wonder that we prefer the Gaussian for
most of the representing we do of the actual world. 

Taleb explains, however, that when it comes to social systems like the economy,
we unfortunately get Black Swans. We get 100 feet tall people occasionally, or
in other words we get unpredictable market _crashes_ . We can't predict when
they'll happen, he says, but we can predict that they'll come around eventually,
and screw everything up. He says that when we create shallow abstractions of
real economic behavior like with credit default swaps and derivatives and other
mathematical representations of the real world, we are guaranteed to get less
predictable behavior and really large anomalies (like 100 feet tall people). So
he says that the economy is not Gaussian.

All of this is well and good but all the computer modeling is based on Gaussian
principles. This is what's called Really Bad, because we're relying on all that
modeling, remember. It means that as we make the economy "digital" with shallow
mathematical abstractions (like default swaps), we also make it more of a "lie",
in so far as the Black Swan feature will tend to get concealed in the layers of
Gaussian computation we're using to make money. All the money is made possible
when we get rid of the rich features of reality, like actual real estate, and
digitize it. If we know that, sooner or later, we're guaranteed to lose all the
money we've made, because the future behavior of these systems contains a Black
Swan, but our computer models assure us that the swans are all white, do we
care? As long as we make the money now, maybe we don't. If we know we're
getting lonely on Facebook but we still have something to do at night with all
of our representations of friends, do we care? It takes some thought to figure
out what we care about and whether we care at all. (It's interesting to ask
whether we start caring, as a rule, only after things seem pretty bad.)

This is the case with the economy, it seems.

So the second big theory problem with the inverted modern world is that
computation is inductive. This is a fancy way of saying that the Safeway
database cannot figure out what I might like unless it's based on what I've
already proven I like. It doesn't know the real me, for one. It knows the
abstraction. And even more importantly, because computation is inductive, it
must always infer something about me or my future based on something known about
me and in my past. Human thought itself is partly inductive, which is why I'll
expect you to show up at around 5 pm at the coffee shop on Thursdays, because
you always do. But I might also know something about you, like say that you're
working at 5. 

Knowing that you're working at 5 on Thursday is called "causal knowledge",
because I know something about you instead of just the past observations of you
showing up. I have some insight about you. It's "causal" because if you work
at 5 on Thursday, that causes you to be there regardless of whether you've shown
up in the past. It's a more powerful kind of knowledge about you. We want our
computers to have insights like this but really, they are more at home with a
database full of entries about your prior arrivals on Thursdays at 5. The
computer really doesn't care or know why you show up. This is induction. 

Induction applies to the Black Swans in stock market crashes because we were all
thinking that "All swans were white" based on our computer models of the past. 
Those models were wrong, it turns out, so we didn't see the Black Swan coming. 
If we hadn't been convinced the computer models were so smart, we might have
noticed the exponential properties of the system. Or: we might have noticed
the inherent, real world volatility that we were amplifying by abstracting it,
and relying on inductive inferences instead of causal knowledge or insight. 
Computers are very good at convincing us we're being very smart about things by
analyzing all those huge data sets from the past. When something not in that
past shows up, they're also very good at making things become chaotic. This is
a reminder that the real world is actually in charge.

It's very complicated to explain why computers don't naturally have the
"insight" or "causal knowledge" part of thinking that we do (and why they can't
really be programmed to have it in future "smarter" versions either). Generally
Artificial Intelligence enthusiasts will insist that computers will get smarter
and eventually will have insights that predict the Black Swans (the very ones
they've also made possible). In general however the Problem of Induction, which
is a kind of blind spot (to go along with the "lie" of abstraction) is part and
parcel of computation. If you combine this inductive blindness with the
shallowness of the models, you get a world that is really good at doing simple
things quickly. If you question whether this is our inevitable future, and
whether perhaps there are entirely new vistas of human experience and culture
available to us (including the technology we make), I think you're on the right
track.

_Here is a representation of me: 42 73 1 M. What does it mean? I once used
something called "log linear" modeling to predict who would pay traffic tickets
in data provided by the state of Iowa (true). We used the models of hundreds of
thousands of people with database entries like this example, but more
complicated, to predict those with greater than some number n likelihood to
never pay. Then we recommended to the state of Iowa not to bother with these
people. It worked pretty well, actually, which is why we make shallow
representations for tasks like this..._ 

<span style="font-size: large;"> W </span> hat's funny about technologists is
how conservative they are. A hundred fifty years ago, the technologists were
passionately discussing the latest, powerful methods for extracting whale oil
from the blubber of Sperm and Baleen whales harpooned and gutted against the
wooden hulls of Atlantic Whalers. No one stopped to wonder whether the practice
was any good, because it seemed inevitable. There was money to be made, too. 
No one even considered that perhaps there was something better, until petroleum
showed up. This is why you see techno-futurists like Kevin Kelly, co-founder of
Wired magazine, or author and futurist Ray Kurzweil always talk as if they can
extrapolate our future in the digital world from observations of the past. They
pretend it's simple and like a computation to see into the future. Kelly is
also eager to explain that technological innovation is not the product of
individual, human insight and genius but rather a predictable and normal
process. The great philosopher of science Karl Popper explained how
technological innovation is intrinsically unpredictable. But you can see that
Kelly and folks like Clay Shirky (Here Comes Everybody, Cognitive Surplus)
already see the future and already have concluded that humans have less and less
to do with it, as digital technology gets smarter and smarter. All these
predictions and all those books sold (and real paper books, too!) would be wrong
if someone just invented a better mouse trap, like people always do. When
petroleum became readily available all the whale-oil-predictions became silly
and retrograde, almost overnight.

If you believe there are no Black Swans and things are moving in a direction,
you don't like these comparisons (do you?). But the real world is messy and
technology is not smart in the way that human minds are, so we have to pretend
if we want to predict the future that's described. When everything is shallow
(abstraction) and quick but limited (induction) you need something to grab onto
to compensate, which is why we say all the computation will get "smarter." If
it doesn't, we're stuck pretending that shallow and quick is human culture. 
That's too hard to do, eventually, which is why we have innovation and why the
Atlantic Whalers eventually became obsolete and why we're due for some different
digital designs than what we have now. I have some thoughts on this, but that's
the subject of another discussion.